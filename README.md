
<img width="1776" height="731" alt="Screenshot 2025-11-25 153020" src="https://github.com/user-attachments/assets/25604800-9212-4800-bf16-a91fc7bc83a2" />
ï»¿<img width="923" height="782" alt="Screenshot 2025-11-25 154811" src="https://github.com/user-attachments/assets/62bb764d-e612-48ba-a8a0-b9c09b2ca0e6" />

 # rag_chatbot

ğŸ“š LetsRAG Chatbot  
### Retrieval-Augmented Generation (RAG) â€¢ FAISS Vector Search â€¢ Local LLM â€¢ PII-Safe â€¢ Streamlit UI

A fully local, end-to-end **Retrieval-Augmented Generation (RAG) chatbot** that takes any PDF, converts it into searchable vector embeddings, retrieves relevant chunks using **FAISS**, and answers user questions using **FLAN-T5** â€” entirely **offline** and **cost-free**.

This project demonstrates strong practical skills in:
- LLM Application Engineering  
- NLP + Embeddings  
- Vector Databases (FAISS)  
- RAG Pipelines  
- Information Retrieval  
- Streamlit UI  
- PII detection & pseudonymization  
- Software Engineering & Modular Design  

---

## ğŸš€ Features

### ğŸ” PDF Ingestion & Processing
- Secure text extraction using PyMuPDF (`secure_ingest.py`)  
- Clean chunking with configurable chunk sizes & overlap (`ingest.py`)

### ğŸ”¤ Embeddings
- Uses **MiniLM-L6-v2** (Sentence Transformers) for lightweight, fast 384-dim embeddings (`embeddings.py`)

### ğŸ“ Vector Search (FAISS)
- Fast similarity search implemented via FAISS IndexFlatL2 (`vectorstore.py`)  
- Stores + retrieves chunks with metadata

### ğŸ¤– Local LLM for QA
- Uses **FLAN-T5-Small** (`query.py`) for grounded answers  
- Outputs detailed 3â€“6 sentence responses built ONLY from context

### ğŸ” PII Handling (Bonus Feature)
- Detects emails, phones, named entities (`pii_utils.py`)  
- Pseudonymizes with secure HMAC hashing (`pseudonymize.py`)  

### ğŸ–¥ Streamlit Frontend
- Document upload  
- Chunk-level debugging view  
- Final answer section  
- Clean UI layout (`main.py`)

---

Architecture

PDF Upload
â”‚
â–¼
Secure Ingestion (PyMuPDF)
â”‚
â–¼
Text Chunking (overlap-aware)
â”‚
â–¼
Embedding Model (MiniLM)
â”‚
â–¼
FAISS Vector Store
â”‚
â–¼
Top-K Context Retrieval
â”‚
â–¼
FLAN-T5 Answer Generation
â”‚
â–¼
Streamlit UI Output



## ğŸ“‚ Project Structure

â”œâ”€â”€ main.py # Streamlit frontend
â”œâ”€â”€ README.md
â”œâ”€â”€ app/
â”‚ â”œâ”€â”€ embeddings.py # MiniLM embeddings
â”‚ â”œâ”€â”€ ingest.py # Chunking + simple extraction
â”‚ â”œâ”€â”€ secure_ingest.py # PyMuPDF extractor
â”‚ â”œâ”€â”€ query.py # FLAN-T5 QA pipeline
â”‚ â”œâ”€â”€ vectorstore.py # FAISS wrapper
â”‚ â”œâ”€â”€ pii_utils.py # PII detection
â”‚ â””â”€â”€ pseudonymize.py # Pseudonymization (HMAC)
â””â”€â”€ temp.pdf (generated on upload)



## âš™ï¸ Installation


2ï¸âƒ£ Create a virtual environment

python -m venv venv
venv\Scripts\activate     # Windows
# OR
source venv/bin/activate # Mac/Linux
3ï¸âƒ£ Install dependencies

pip install -r requirements.txt
â–¶ï¸ Running the Application

streamlit run main.py
Then open:
http://localhost:8501

Usage
Upload any PDF

The system extracts and chunks the text

FAISS stores embeddings for fast similarity search

You ask a question

Top-K chunks are retrieved

FLAN-T5 generates a grounded, context-aware answer

Example Demo
User Question:
â€œWhat is Docker?â€

Retrieved Chunks:
Shows chunk data for transparency

Answer:
Generated by FLAN-T5 using ONLY PDF context.

PII Protection (Optional Module)
This project includes built-in PII detection & pseudonymization:

Detects:

Emails
Phone numbers
PERSON
ORG
DATE
LOCATION

Replaces with deterministic HMAC-hashed tokens like:
user_aj2ns9ab
Enable it inside your ingestion pipeline if needed.


Future Enhancements
Multi-PDF indexing
Chat history memory
Model selection (FLAN-T5-Large, Llama-3, Qwen)
Docker deployment
GPU acceleration
Hybrid embeddings (dense + sparse)
Online/Offline toggle

If You Found This Useful
Give the repo a â­ on GitHub!


